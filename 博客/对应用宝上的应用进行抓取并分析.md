---
title: 对应用宝上的应用进行抓取并分析
date: 2017-01-26 21:30:41
categories: 技术
tags: Crawler
---
我们用Python编写一个爬虫，抓取应用宝官网上全部的应用数据，然后对数据进行清洗和统计，得出下载量排名前30的应用。
<!--more-->
## 运行前的准备

- python 3.5.1
- MongoDB 3.2

## 解决依赖

- bs4
- requests
- lxml
- pymongo

## 爬虫和数据清洗代码

```
import requests
from bs4 import BeautifulSoup
import pymongo
import re
import lxml

# 使用正则来匹配数字
MODE_FLOAT = re.compile(r'\d+.\d+')
MODE_INT = re.compile(r'\d+')

headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.106 Safari/537.36"}

# 连接至数据库
client = pymongo.MongoClient()
db = client['MyAPP']
col = db['sj']

# 避免数据重复
name_set = set()

# 储存数据
def store(data_list):
    for data in data_list:
        data_dict = {}
        if data[0] in name_set:
            continue
        name_set.add(data[0])
        data_dict['Name'] = data[0]
        data_dict['DownloadCount'] = data[1]
        data_dict['Category'] = data[2]
        col.insert_one(data_dict)

# 请求和分析AJAX表单
def get_ajax_data(union_id):
    request_url = "http://sj.qq.com/myapp/union/apps.htm?unionId=" + union_id
    raw_ajax_data = requests.get(request_url, headers=headers).json()
    ajax_data = raw_ajax_data['obj']
    return ajax_data

# 获取BeautifulSoup对象
def get_soup(url):
    r = requests.get(url, headers=headers)
    soup = BeautifulSoup(r.text, "lxml")
    return soup

# 处理下载量数据
def deal_count(str):
    try:
        count = float(MODE_FLOAT.findall(str)[0])
    except IndexError:
        count = float(MODE_INT.findall(str)[0])

    if '万' in str:
        count *= 10000
    elif '亿' in str:
        count *= 100000000
    return int(count)

# 爬虫
def crawl(url):
    soup = get_soup(url)
    area = soup.find_all("li", {"class": "union-list  nopicshow J_Mod"})
    datas = []
    for part in area:
        sections = part.find_all("section", {"class": "union-list-app"})
        for section in sections:
            detail = section.find("div", {"class": "union-list-app-detail"})
            name = detail.find("a", {"class": "appName ofh"}).get_text()
            category_name = "none"
            raw_download = detail.find("span", {"class": "download"}).get_text()
            down_count = raw_download.replace("\r\t", "").replace("\r", "").replace(" ", "").replace("\n\t", "").replace("\n", "")
            count = deal_count(down_count)
            datas.append((name, count, category_name))
        info = part.find("div", {"class": "union-data-box"})
        idx = info.find("a").attrs['idx']
        ajax_data = get_ajax_data(idx)
        if ajax_data == None:
            continue
        for i in ajax_data:
            name = i['appName']
            down_count = i['appDownCount']
            category_name = i['categoryName']
            datas.append((name, down_count, category_name))
    store(datas)

def run():
    for i in range(1, 9):
        print(i)
        start_url = "http://sj.qq.com/myapp/union.htm?orgame=1&typeId=&page=" + str(i)
        crawl(start_url)

if __name__ == "__main__":
    run()
```

## 数据分析代码

```
import pymongo
import numpy as np

client = pymongo.MongoClient()
db = client['MyAPP']
col = db['sj']

raw_datas = list(col.find())

datas = np.array([(x['Name'], x['DownloadCount']) for x in sorted(raw_datas, key=lambda i:i['DownloadCount'], reverse=True)])

print(datas[:30])
```

## 运行结果

```
[['QQ' '4900000000']
 ['微信' '3200000000']
 ['WiFi万能钥匙' '2200000000']
 ['腾讯手机管家—清理垃圾防骚扰' '1576306480']
 ['QQ空间' '1300000000']
 ['腾讯视频 - W两个世界独播' '1292695993']
 ['酷狗音乐' '1261264169']
 ['QQ浏览器-正版书籍品读季' '1129564448']
 ['爱奇艺' '949267305']
 ['手机淘宝' '932934483']
 ['QQ音乐' '899130080']
 ['腾讯新闻' '857553784']
 ['优酷视频播放器' '800000000']
 ['百度地图' '786650865']
 ['天天酷跑' '740434831']
 ['搜狗输入法' '734358637']
 ['新浪微博' '721268878']
 ['猎豹清理大师' '518558466']
 ['最美天气' '510489524']
 ['今日头条（新闻阅读）' '509999999']
 ['酷我音乐播放器' '479764961']
 ['WPS Office' '444314760']
 ['高德地图（快捷导航版）' '433993857']
 ['欢乐斗地主（腾讯）' '420000000']
 ['美团-团购美食电影酒店优惠' '400000000']
 ['铃声多多' '390000000']
 ['快手' '370000000']
 ['墨迹天气' '365004212']
 ['美图秀秀' '321390703']
 ['乐视视频' '319078125']]
```